# This repo contains a set of notes for setting up Strix Halo box ...
... to run local LLMs using kernel 6.18 + rocm 7.2.

Contents:
- [ubuntu setup notes](setup.md)
- [Docker/Podman container with llama.cpp/whisper.cpp/llama-swap](toolboxes/) that exposes models though OpenAI-compatible API and could be an alternative to ollama, for example.
